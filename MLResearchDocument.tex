
% --------------------------------------------------------------
% This is all preamble stuff that you don't have to worry about.
% Head down to where it says "Start here"
% --------------------------------------------------------------
\documentclass{article} % For LaTeX2e
\usepackage{nips14submit_e,times}
\usepackage{amsmath,amsthm,amssymb}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}
\usepackage{verbatim}
\usepackage{color}
\usepackage[toc,page]{appendix}
\usepackage{algorithm}
\usepackage{algpseudocode}


\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newenvironment{theorem}[2][Theorem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{lemma}[2][Lemma]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{exercise}[2][Exercise]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{problem}[2][Problem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{question}[2][Question]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{corollary}[2][Corollary]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}

\DeclareMathOperator*{\argmax}{arg\,max}

% --------------------------------------------------------------
% Start here
% --------------------------------------------------------------
\title{Applications of Reinforcement Learning in Robotics and Optimal Control}%replace X with the appropriate number

\author{
Matthew Sheckells\\
Department of Computer Science\\
Johns Hopkins University\\
\And
Kristopher Reynolds \\
Department of Mechanical Engineering\\
Johns Hopkins University\\
}



\nipsfinalcopy
\begin{document}



\maketitle

\section{Introduction}
Concepts of machine learning are very much at the forefront of modern research interests. However, the popular supervised learning framework does not account for environments that are constantly in flux. This setting is relevant in particular to robotic vehicles navigating unknown terrains. A notion that suggests a means to handle these changing environments is reinforcement learning (RL). Unlike supervised learning, RL actively receives information by engaging an environment through \textit{actions}. By doing so, two types of information are rendered: the current \textit{state}, and some notion of a quantifiable \textit{reward}. This reward is related to an a priori defined task that has a corresponding goal. RL aims to learn reward values for taking an action from a particular state, so that a system can learn to act optimally in a given situation.

The fields of robotics and optimal control have benefitted from concepts of reinforcement learning \cite{kober_reinforcement_2013}\cite{kaelbling_reinforcement_1996}. Several robotic systems in  \cite{bhasin_reinforcement_2011} and \cite{hester_rtmba:_2012} have implemented this technique, showing that performance equalled and in some cases exceeded that of hand-tuned controllers. The advantage of RL is that it is robust to changes in a system, such as malfunctioning actuators, changes in models parameters, and external disturbances. Optimal control is similar in several ways to RL. Moreover, both have some notion of finding a \textit{policy} that optimizes some objective function. For optimal control, this is minimizing some cost, whereas for RL, it is maximizing a reward. For robotics-based RL, there are several challenges that need to be addressed in order for RL to be realized.
\begin{itemize}
\item Continuous State and Action Space: Robots have sensors that output data in a continuous domain.  Likewise, robots themselves act in a continuous environment. This issue is addressed in \cite{gaskett_thesis} and \cite{q_learning_navigation}.
\item Under-modeling and Uncertainty: The full state may not be observable, and noise requires some method of state estimation.
\item Goal specification: Crafting reward functions (i.e. \textit{reward shaping}) may be difficult since rewards can rely upon successful completion of a task. This may lead to a sparsity of instances actually influencing the learning of a robotic system.
\end{itemize}

The outline of the document is the following UPDATE THIS!!!. First, the popular Q-learning algorithm will be  defined. Next, a rendition of Q-learning called delayed Q-learning will be presented with a proof sketch showing that it is Probably Approximately Correct in Markov Decision Processes (i.e. PAC-MDP), which is a slightly different notion from general PAC learnability (more on this later). It will then be discussed how Q-learning is handled in continuous state/action spaces, which is relevant to robotics applications since modeling is generally done so in a continuous setting. An algorithm called the wire fitted neural network (WFNN) will be presented with a proof that this approach is indeed PAC learnable. 

\section{Notation and Assumptions of Reinforcement Learning}

Reinforcement Learning (RL) has been defined as the study of planning and learning in a scenario where a learner actively interacts with the environment to achieve a certain goal \cite{mohri}. The learner, or agent, collects information through a course of actions by interacting with the environment.  The set of all possible actions is defined as $a \in A$ and the set of all possible states is $s \in S$. For each action that the agent takes, two pieces of information are obtained: a reward $r(s,a)$ for executing $a$ while in $s$, and a new state in the environment $s'$.  The objective of the agent is to choose actions which maximize its reward and hence help it achieve its goal.  Since the agent only receives rewards related to the action just taken, some other mechanism must be used to account for long-term reward feedback.
The agent must deem whether it is more appropriate to explore further unknown states $s \in S$ and actions $a \in A$ or utilize the information already collected in order to optimize (maximize) the current reward $r(s,a)$. This tradeoff if formally referred to as \textit{exploration vs. expoitation}. RL differs from supervised learning in that there is no distribution from which samples (or features) are drawn. Rather it is through some \textit{policy} given as the mapping

$$
\pi : S \rightarrow A
$$
that this distribution is defined. Another more intuitive concept of RL is that immediate rewards are incrementally extracted through actions made with some environment (as opposed to a batch collection and training scenario). This incremental obtainment of information is more reflective of human nature in that we tend to interact with our environment and learn based off the actions we make and information we receive. An example would be driving over a pothole in the road, and realizing that this negative reward will influence actions taken in the future (i.e. perhaps anticipate this pothole or take a different route). This policy can be quantified in terms of what is known to be a \textit{policy value}, $V_{\pi}$ which is given as 
$$
V_{\pi} = E[\sum_{\tau = 0}^{T-t}r(s_{t+\tau},\pi(s_{t+\tau}))|s_t = s].
$$
Qualitatively, this policy value is the expected reward $r(s,a)$ parameterized by a state $s$ and action $a$ that is yielded when starting at $s$ using policy $\pi$. It is summed over a finite time horizon. The \textit{optimal policy} is in terms of the policy value, moreover $V^{*}=V_{\pi^*}(s) = \max_{\pi} V_{\pi}(s)$. Therefore $V^{*}$ is the optimal cummulative reward that the learner may expect to receive while starting at $s$. The next relevant quantity is the optimal state-action value function $Q^*(s)$, which is defined for all $(s,a) \in S \times A$ as the expected return for taking action $a$ at state $s$ and then following the optimal policy.  It is given in terms of the optimal policy value as
$$
Q^*(s,a) = E[r(s,a)] + \gamma \sum_{s' \in S}Pr[s' | s,a]V^*(s').
$$
The optimal state-action value function can be defined implicitly as
$$
Q^*(s,a) = E_{s'}[r(s,a) + \gamma \max_{a \in A}Q^*(s,a)]
$$
The optimal policy values are related to $Q^*(s,a)$ as 
$$V^*(s) = \max_{a\in A} Q^*(s,a)$$
and the optimal policy is given as 
$$\pi^*(s) = \argmax_{a\in A} Q^*(s,a).$$
Thus, some methods seek to approximate $Q^*$ by exploring the state-action space and thereby obtain $\pi^*$.
The next section defines one such algorithm called Q-learning algorithm that seeks out this optimal state-action value function. 
\section{Q-learning}
Q-learning is an algorithm where the objective is to estimate the optimal state-action value function $Q^*$, when the model of the environment is unknown or changing. It consists of the following steps.  First, sample a new state $s'$ by executing action $a$.  Then, update the state-action value function using the following update:
$$Q(s,a) \leftarrow \alpha Q(s,a) + (1-\alpha)[r(s,a) + \gamma \max_{a' \in A}Q(s', a')],$$
where the parameter $\alpha$ is a function of the number of visits to state $s$ and $\gamma \in [0,1)$ is a factor used to discount future rewards.  A more detailed description of the procedure is shown in Algorithm \ref{qlearning}, where SelectAction($\pi,s$) must ensure that each state can be visited infinitely many times, like in an $\epsilon$-greedy approach.  That is, a random action is chosen with probability $\epsilon$, and the action is chosen as $\argmax_{a\in A} Q(s,a)$ with probability $1-\epsilon$.
\begin{algorithm}
 \caption{Q-learning}\label{euclid}
  \begin{algorithmic}[1]
    \Procedure{Q-learning}{$\pi$}
      \State $Q\gets Q_0$
      \For{\texttt{t=0; t<T; t++}} 
        \State $s \gets $ SelectState($\pi,s$)
        \For{each step of epoch $t$} 
        	\State $a \gets$ SelectAction($\pi,s$)
        	%\If{ \texttt{rand()} $< \epsilon$}
        	%	\State $a \gets $\texttt{rand}$(A)$
        	%\Else
        	%	\State $a \gets \argmax_{a\in A} Q(s,a)$
        	%\EndIf
        	\State $r' \gets$ Reward($s,a$)
        	\State $s' \gets$ NextState($s,a$)
        	\State $Q(s,a) \gets Q(s,a) + \alpha[r' + \gamma\max_{a'}Q(s',a')-Q(s,a)]$
        	\State $s \gets s'$
        \EndFor
      \EndFor
      \State \Return{Q}
    \EndProcedure
  \end{algorithmic}
  \label{qlearning}
\end{algorithm}

\subsection{Proof of Convergence of Q-learning}
The Q-learning algorithm has been shown to converge to the optimal value $Q^*$ with probability one \cite{mohri}.  Here, we show an outline of the proof adapted from \cite{mohri}.  
\begin{proof}
Let $(Q_t(s,a))_{t\geq 0}$ denote the sequence of state-action value functions at $(s,a) \in S \times A$ generated by the algorithm.  By definition of the Q-learning updates,
$$
Q_{t+1}(s_t,a_t) = Q_t(s_t,a_t) + \alpha[r(s_t,a_t) + \gamma \max_{a'}Q_t(s_{t+1},a') - Q_t(s_t,a_t)
$$
which can be recast as the following $\forall s \in S$ and $a \in A$.
$$
Q_{t+1}(s,a) = Q_t(s,a) + \alpha_t(s,a)[r(s,a) + \gamma E_{s'}[\max_{a'}Q_t(s',a')] - Q_t(s,a)] +
$$
$$
\gamma\alpha_t(s,a)[\max_{a'}Q_t(s',a') - E_{s'}[\max_{a'}Q_t(s',a')] ].
$$
Suppose $\alpha_t(s,a) = 0$ when $(s,a) \neq (s_t,a_t)$, and $\alpha_t(s_t,a_t)$ otherwise. Moreover let $\textbf{Q}_t$ have components $Q_t(s_t,a_t)$, and the vector $\textbf{w}_t$ be defined by its $s'^{th}$ component as
$$
w_t(s') = \max_{a'}Q_t(s',a') - E_{s'} [\max_{a'}Q_t(s',a') ].
$$
Also, the vector $\textbf{H}(\textbf{Q}_t)$ has components
$$
\textbf{H}(\textbf{Q}_t)(x,a) = r(s,a) + \gamma E_{s'}[\max_{a'}Q_t(s',a') ]
$$
Inserting this into our previous expression for $Q_{t+1}(s,a)$ (now vectorized) gives
$$
\forall (s,a) \in S \times A, \textbf{Q}_{t+1}(s,a) = \textbf{Q}_t(s,a) + \alpha_t(s,a)[\textbf{H}(\textbf{Q}_t)(s,a) - \textbf{Q}_t(s,a) + \gamma\textbf{w}_t]
$$
In $\cite{mohri}$ it is shown that by definition of $\textbf{w}_t$, $E_{s'}[\textbf{w}_t | F_t] = 0$, and for any $s' \in S$, $|\textbf{w}_t|$ can be upper bounded in terms of $\textbf{Q}_t$ as
\begin{align*}
|\textbf{w}_t(s')| &\leq \max_{a'}|Q_t(s',a')| + |E_{s'}[\max_{a'}Q_t(s',a')] | \\
& \leq 2\max_{s'}|\max_{a'}Q_t(s',a')|\\
&= 2|||\textbf{Q}_t||_{\infty}
\end{align*}
From this, it can be inferred that
$$
E[\textbf{w}^2_t(s) | F_t] \leq 4|||\textbf{Q}_t||^2_{\infty}
$$
Lastly $\textbf{H}$ is a $\gamma$-contraction for $||.||_{\infty}$ since
\begin{align*}
|\textbf{H}(\textbf{Q}_2)(x,a) - \textbf{H}(\textbf{Q}_1)(x,a) & = |\gamma E_{s'} [\max_{a'}Q_2(s',a') - \max_{a'}Q_1(s',a')] |\\
&\leq \gamma E_{s'}[ ~|\max_{a'}Q_2(s',a') - \max_{a'}Q_1(s',a')|~]\\
&\leq \gamma E_{s'}\max_{a'}[~|Q_2(s',a') - Q_1(s',a')|~]\\
&\leq \gamma \max_{s'}\max_{a'}[~|Q_2(s',a') - Q_1(s',a')|~]\\
&= \gamma||\textbf{Q}''_2 - \textbf{Q}'_1||_{\infty}
\end{align*}
$H$ being a $\gamma$-contraction implies that it admits a fixed point $Q^*$ such that $\textbf{H}(\textbf{Q}^*) = \textbf{Q}^*$. This then sufficiently shows that $\textbf{Q}_t \rightarrow \textbf{Q}^*$


\end{proof}

These convergence results, however, make no guarantee about the performance of Q-learning in regard to the number of steps taken in the environment.  In the next section, we discuss an adaptation of Q-learning that provably converges to an $\epsilon$-optimal policy within some number of time steps that is polynomially bounded by the size of the action and state spaces and other relevant quantities.

\section{Delayed Q-learning}
A variant of Q-learning, called Delayed Q-learning, was the first to show that efficient RL is possible even when the model of the MDP is not known or learned.  Its creators proved it to be an RL equivalent of PAC, called Probably Approximately Correct in Markov Decision Processes (PAC-MDP) \cite{strehl_pac_2006}.  If an algorithm is PAC-MDP, then the number of timesteps that it takes for a policy to become $\epsilon$-optimal for all states should be less than some polynomial in the quantities $(\left|S\right|,\left|A\right|,1/\epsilon,1/\delta,1/(1-\gamma))$ with probability at least $1-\delta$ \cite{kakade}.


Delayed Q-learning differs from traditional Q-learning in a few ways.  Rather than updating the Q-values at every time step, the algorithm updates $Q(s,a)$ when a state-action pair $(s,a)$ has been executed $m$ times since its last update.  It also uses a different update function:
$$Q_{t+1}(s,a) = \frac{1}{m}\sum_{i=1}^m(r_{k_i}+\gamma V_{k_i}(s_{k_i})) + \epsilon_1$$  
where $r_i$ is the $i$th reward received and $s_{k_1},\dots,s_{k_m}$ are the $m$ most recent next-states observed form executing $(s,a)$ at times $k_1 < \dots < k_m = t$, respectively. The user defines $\epsilon_1$ and $m$.  After $m$ executions of $(s,a)$ an update will only occur if the new Q-value estimate is at least $\epsilon_1$ smaller than the previous estimate.  If it is not smaller, then updates of $Q(s,a)$ are no longer allowed until another Q-value estimate (for a different state-action pair) is updated.  When the algorithm begins, $Q(s,a)$ is initialized to $1/(1-\gamma)$.

Waiting for $m$ samples before updating has an averaging effect that helps minimize any randomness. Adding $\epsilon_1$ to the update function encourages optimistic Q-value estimates which in turn facilitates safe exploration. By starting with high initial Q-values and only allowing Q-values to decrease ensures that each state is regularly visited and that each action is tried from every state.  This makes sure each state-action pair receives enough samples to converge to the correct Q-value.  In the following section, we outline the proof that shows Delayed Q-learning is PAC-MDP.


\subsection{Proof Sketch: Delayed Q-learning is PAC-MDP}

The full proof is complex, so we mention only the key insights.  The full proof can be found in \cite{strehl_pac_2006}.  

As noted previously, delaying updates for $m$ samples has an averaging effect.  This ensures with high probability that each update is a smart thing to do.  Furthermore, we expect an attempted update of $Q(s,a)$ to succeed so long as it is very inconsistent with the other value function estimates.  By starting with high initial estimates for the Q-values and only allowing them to decrease, the algorithm ensures that each state-action pair will be sampled enough times for its Q-value to converge.  Once the estimate becomes $\epsilon$-close to its true value, the algorithm has a mechanism to turn off the learning of a particular state-action pair.  This guarantees that only a finite number of Q-value updates occur.  The proof uses these key properties of the algorithm to that Delayed Q-learning will follow an $\epsilon$-optimal policy on all but $$O\left(\frac{SA}{\epsilon^4(1-\gamma)^8}\ln{\frac{1}{\delta}}\ln{\frac{1}{\epsilon(1-\gamma)}}\ln{\frac{SA}{\delta\epsilon(1-\gamma)}}\right)$$ steps with probability at least $1-\delta$ for a particular choice of $m$ which minimizes the bound.

\section{Q-learning in Robotics}
Robotic systems are a central application area of Q-learning.  The following sections discuss the key challenges of applying Q-learning to the control of robotic systems.  In particular, we discuss how Q-learning has been adapted to work in continuos state and action spaces and how rewards are typically defined to achieve optimal control. 

\subsection{Q-learning with Continuous States and Actions}
As we have discussed, in traditional Q-learning, the state and action spaces of a system are discretized and finite, with a single Q-value assigned to each state-action pair.  Since robots act in the real world where measurements are continuously valued, it is more natural to represent states in a continuous domain.  Moreover, this alleviates any issues with perceptual aliasing, where two similar different states are considered the same in a discretized space.  One could attempt to approximate a continuous space by finely discretizing it, but this increases the amount of memory required to store $Q$-values and the learning time for Q-learning becomes intractable.  In addition, in a grid-based representation there is no notion of similarity between neighboring states, so discrete representations do not generalize well.  

Generalization can be introduced by mapping states to action-values via a function approximator.  Several existing systems use a neural network as the function approximator \cite{lin} \cite{deep_rl} \cite{gaskett_thesis} with success. In \cite{lin} and \cite{deep_rl}, each possible action has an associated neural network which approximates the reward of executing that action from the state input to the neural net.  The neural networks were trained to output the correct action-values based on the rewards received from acting in the environment.  

These methods still, however, rely on a discretized action space.  This means that an agent can only act in a finite number of predetermined ways, meaning its chances of acting optimally (with respect to the underlying continuous action space) is small.  Like the state space, discretizing the action space has its drawbacks: high memory use and poor generalization.  Furthermore, using continuous actions gives more accurate and predictable movement, reduces wear, and improves energy consumption in practice\cite{gaskett_thesis}. 

There exists a number of approaches for incorporating both continous state and action spaces into Q-learning.  In \cite{takahashi}, states and actions are coarsely discretized, and linear interpolation is used to represent intermediate states.  In \cite{gaskett_thesis}, a single neural network takes the curren state as input, and the outputs correspond to control points of an action-reward interpolator.  The Q-value of any given action can be smoothly, approximately computed using the interpolator.  

Actor-critic methods contain two separate, interacting subsystems.  One estimates the long-term utility for each state and the other learns to choose the optimal action in each state.  \cite{bhasin_reinforcement_2011} describes such a system for RL-based optimal control of uncertain nonlinear systems, where the actor and critic each consist of a neural network.  Since neural networks are so widely adopted in RL, we show proof that they are PAC-learnable, and hence a good choice of function approximator, in Section 5.2.

\cite{carden2014} uses Nadaraya-Watson kernel regression \cite{nadaraya} to estimate the Q-value of a state-action pair based on past experiences and shows convergence to the true Q-values within some tolerance with probability 1.  This type of regression estimates the value at a particular point as a weighted average of nearby observations using non-negative kernel function to assign weights to observations.  Its proof of convergence is quite technical, so we briefly discuss the key details.  It follows very similarly to proof of convergence for traditional Q-learning given by Watkins \cite{watkins}, which uses an artifically controlled finite-length MDP called an Action-Replay Process (ARP) for the purposes of the proof.  The action space of the ARP is equivalent to that of the actual process MDP and the state space of the ARP is built from the state space of the actual MDP.   The ARP replays sequences of actions in such way so that the optimal Q-values are known for the ARP. Watkins shows that as the length of the ARP approaches infinity, the ARP's transition probabilties and expected rewards  converge to those of the real process.  In addition, bounds put on the learning rate (the sum of rates must tend to inifinity while the sum of squares of rates must be less than inifinity) ensure this convergence with probability one.  Since the ARP tends towards the real process, so do its optimal Q-values.  \cite{carden2014} makes an additional step and shows that the expected rewards for the ARP are equivalent to the kernel regression estimates. 

\subsection{Learning Bounds on Neural Nets}
Generalization bounds can be formulated for multilayered neural networks (NN) through inequalities involving the growth function $\Pi_F(m)$, where $F$ is a function class. First some nomenclature for the NN classifier will be presented. Let $k$ denote the $k^{th}$ node in a neural network. For binary classification, the activation function $\sigma$ is defined as
$$
\sigma(t) = sgn(t)
$$
where
$$
t = w_k^Tx - \theta_k.
$$
The input space $X = \mathbb{R}^d$, and in general a multilayer neural network with $l$ layers has the functional composition
$$
f(x) = f_l \circ \cdots \circ f_2 \circ f_1 (x)
$$
where
$$
f_i : \begin{cases}
    \mathbb{R}^d_{i-1} \rightarrow \{-1,1\}^{d_i}, & \text{if 1 $\leq$ i $\leq$ $l$}.\\
    \mathbb{R}^d_{i-1} \rightarrow \{-1,1\}, & \text{if i = $l$}
  \end{cases}
$$
The notion of a component function will now be presented as
$$
f_{i,j}(x) = sgn(w_{i,j}^Tx - \theta_{i,j}) \in \{-1,1\}
$$
and $x,w_{i,j} \in \mathbb{R}^d_{i-1}$, $\theta_{i,j} \in \mathbb{R}$. The variable $d_{i-1}$ is the dimension of layer $i-1$.
\\
\\
The growth function is defined to be
$$
\Pi_H(m) \doteq \max_{\{x_1,\cdots,x_m\} \subseteq X} |\{h(x_1),\cdots,h(x_m) : h \in H \} |
$$
where $H$ is a hypothesis class. In particular, let $H \rightarrow F$, and consider the function classes $F_1$ and $F_2$ as well as their Cartesian product $F_1 \times F_2$ and composition $F = F_1 \circ F_2$. The following lemmas will deem useful.
\\
\\
\textbf{Sauer's Lemma}
\\
Let $d$ = VCDim($H$), where $H$ is a hypothesis class. Then $\forall m \geq d$, the growth function is upper bounded as.
$$
\Pi_H(m) \leq \begin{pmatrix}
\frac{m}{d}^de^d
\end{pmatrix}
$$
\\
\\
\textbf{Growth Function Lemma}
$$
\Pi_F(m) \leq \Pi_{F_1}(m)\Pi_{F_2}(m)
$$
where $F$ is the composition $F$ = $F_1 \circ F_2$.
\\
\\
Now consider the function composition
$$
F = F_l \circ F_{l-1} \circ \cdots F_2 \circ F_1
$$
This implies via the above growth function lemma

\begin{align*}
\Pi_F(m) &\leq \prod_{i=1}^l \Pi_{F_i}(m)\\
& \leq \prod_{i=1}^l\prod_{j=1}^{d_i} \Pi_{F_{i,j}}(m)\\
\end{align*}
Furthermore using Sauer's lemma on a component function gives
\begin{align*}
\Pi_F(m)& \leq \prod_{i=1}^l\prod_{j=1}^{d_i} \Pi_{F_{i,j}}(m)\\
& \leq \prod_{i=1}^l\prod_{i=1}^{d_i}(\frac{me}{d_{i-1}+1})^{d_{i-1}+1}\\
\end{align*}
It well known that the VC dimension of halfspaces in $d$ dimensions is $d +1$, which is why $d+1$ manifests itself in Sauer's lemma.
The generalization bound given in \cite{mohri} is 
$$
R(h) \leq \widehat{R}(h) + \sqrt{\frac{2\log{\Pi_H{(m)}}}{m}} + \sqrt{\frac{\log{\frac{1}{\delta}}}{2m}}.
$$
Defining the variable $N$, which is the total number of parameters in the NN as 
$$
N = \prod_{i=1}^l\prod_{i=1}^{d_i}(\frac{me}{d_{i-1}+1})^{d_{i-1}+1}
$$ 
gives a nicer (but looser) bound on the growth function as
$$
\Pi_H{(m)} = me^N
$$
Thus we have with probability at least 1 - $\delta$
$$
R(h) \leq \widehat{R}(h) + \sqrt{\frac{2\log{me^N}}{m}} + \sqrt{\frac{\log{\frac{1}{\delta}}}{2m}}
$$

\subsection{Reward Crafting for Optimal Control}
Optimal control theory deals with deriving control policies which aim to minimize some cost specified by the user.  This is rather similar to goal of Q-learning which aims to maximize the reward received by an agent over some time-horizon.  In fact, \cite{sutton} claims that reinforcement learning is equivalent to direct adaptive optimal control.  When an accurate model of the system to be controlled is not available, adaptive optimal control methods can be applied to trajectory tracking and optimal control scenarios.  In optimal control a cost is typically given as 
$$J = \phi(x(t_f),t_f) + \int_{t_0}^{t_f} L(x(t),u(t),t)dt$$
where $x(t)$ is the state trajectory, $u(t)$ is the control sequence, and $\phi$ is a terminal cost.  An equivalent RL reward can be given as $-J$.  A natural choice for $L$ then is $L = \frac{1}{2}x(t)^TQx(t) + u(t)^TRu(t)$, where $Q$ and $R$ are positive definite matrices.  This quadratic cost is often associated with the Linear Quadratic Regulator (LQR) \cite{lqr} and is employed by Ng \cite{ng_autonomous_2006} for RL-based stabilization of an autonomous helicopter.  This choice of reward encourages the system to remain close to the origin (i.e. stabilizes the system) while also encouraging small control outputs and smooth control transitions.  Ng also discusses in \cite{ng_thesis} a potential-based method for specifying rewards that can lead to significant speedups of the learning process.

\section{Conclusion}
This investigation of RL in the context of robotics and optimal control has further influenced the research areas of the authors of this document. Through the presentation/discovery of tractable RL algorithms, it now seems feasible that some of these techniques may be approrpiate for solving problems of motion planning, state estimation, trajectory generation, and navigation all of which are native to the RPK (Chirikjian) and the ASCO (Kobilarov) labs at Johns Hopkins University.


\nocite{yang_multiagent_2004}
\nocite{kim_autonomous_2003}
\nocite{mohri_foundations_2012}

\newpage
\bibliography{ML}{}
\bibliographystyle{plain}




% --------------------------------------------------------------
% You don't have to mess with anything below this line.
% --------------------------------------------------------------
\end{document}

