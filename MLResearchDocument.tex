% --------------------------------------------------------------
% This is all preamble stuff that you don't have to worry about.
% Head down to where it says "Start here"
% --------------------------------------------------------------
\documentclass[10pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amsthm,amssymb}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}
\usepackage{verbatim}
\usepackage{color}
\usepackage[toc,page]{appendix}



\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newenvironment{theorem}[2][Theorem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{lemma}[2][Lemma]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{exercise}[2][Exercise]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{problem}[2][Problem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{question}[2][Question]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{corollary}[2][Corollary]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\begin{document}
% --------------------------------------------------------------
% Start here
% --------------------------------------------------------------
\title{Applications of Reinforcement Learning in Robotics and Optimal Control}%replace X with the appropriate number
\author{\\ %replace with your name
Kristopher L. Reynolds\\
Matthew C. Sheckells
\\} %if necessary, replace with your course title
\maketitle
\section{Introduction}
The fields of robotics and optimal control have benefitted from concepts of reinforcement learning (RL) \cite{kober_reinforcement_2013} \cite{kaelbling_reinforcement_1996}. From an optimal control standpoint, the objective is to minimize some cost analogously to how RL aims to maximize a reward.  Several robotic systems in  \cite{bhasin_reinforcement_2011} and \cite{hester_rtmba:_2012} have all successfully implemented this notion and shown that performance equalled and in some cases exceeded that of hand-tuned controllers. The advantage of RL is that it is robust to changes in a system, such as malfunctioning actuators, changes in models parameters (i.e. inertia), and external disturbances. 

\section{Q-Learning}
\subsection{PAC-Learnability of Q-Learning}

\nocite{yang_multiagent_2004}
\nocite{kim_autonomous_2003}

\newpage
\bibliography{ML}{}
\bibliographystyle{plain}




% --------------------------------------------------------------
% You don't have to mess with anything below this line.
% --------------------------------------------------------------
\end{document}