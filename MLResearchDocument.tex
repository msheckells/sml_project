% --------------------------------------------------------------
% This is all preamble stuff that you don't have to worry about.
% Head down to where it says "Start here"
% --------------------------------------------------------------
\documentclass[10pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amsthm,amssymb}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}
\usepackage{verbatim}
\usepackage{color}
\usepackage[toc,page]{appendix}



\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newenvironment{theorem}[2][Theorem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{lemma}[2][Lemma]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{exercise}[2][Exercise]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{problem}[2][Problem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{question}[2][Question]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{corollary}[2][Corollary]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\begin{document}
% --------------------------------------------------------------
% Start here
% --------------------------------------------------------------
\title{Applications of Reinforcement Learning in Robotics and Optimal Control}%replace X with the appropriate number
\author{\\ %replace with your name
Kristopher L. Reynolds\\
Matthew C. Sheckells
\\} %if necessary, replace with your course title
\maketitle
\section{Introduction}
Concepts of machine learning are very much at the forefront of modern research interests. However, the popular supervised learning framework does not account for environments that are constantly in flux. This setting is relevant to robotic vehicles navigating unknown terrains. A notion that suggests a means to handle these changing environments is reinforcement learning (RL). Unlike supervised learning, RL actively receives information by engaging an environment through \textit{actions}. By doing so, two types of information are rendered: the current \textit{state}, and some notion of a quantifiable \textit{reward}. This reward is related to an a priori defined task that has a corresponding goal. RL aims to learn reward values for taking a particular action from a particular state, so that a system can learn to act optimally in a given situation.

The fields of robotics and optimal control have benefitted from concepts of reinforcement learning \cite{kober_reinforcement_2013}\cite{kaelbling_reinforcement_1996}. Several robotic systems in  \cite{bhasin_reinforcement_2011} and \cite{hester_rtmba:_2012} have implemented this technique, showing that performance equalled and in some cases exceeded that of hand-tuned controllers. The advantage of RL is that it is robust to changes in a system, such as malfunctioning actuators, changes in models parameters, and external disturbances. Optimal control is similar in several ways to RL. Moreover, both have some notion of finding a \textit{policy} that optimizes some objective function. For optimal control, this is minimizing some cost, whereas for RL, it is maximizing a reward. For robotics-based RL, there are several challenges that need to be addressed for RL to be realized.
\begin{itemize}
\item Continuous State and Action Space: Robots have sensors that output data in a continuous domain.  Likewise, robots themselves act in a continuous environment. This issue is addressed in \cite{gaskett_thesis} and \cite{q_learning_navigation}.
\item Under-modeling and Uncertainty: The full state may not be observable, and noise requires some method of state estimation.
\item Goal specification: Crafting reward functions (i.e. \textit{reward shaping}) may be difficult since rewards can rely upon successful completion of a task. This may lead to a sparsity of instances actually influencing the learning of a robotic system.
\end{itemize}
The outline of the document is the following. First, the popular Q-learning algorithm will be  defined.
\section{Q-learning}
Q-learning is an algorithm where the objective is to estimate the optimal state-action value $Q^*$, where the model of interest is unknown. The optimal policy is given by
$$
Insert Equation here
$$

A variant of the algorithm, called Delayed Q-learning, has been shown to by Probably Approximately Correct in Markov Decision Processes (PAC-MDP) \cite{strehl_pac_2006}.
\subsection{Delayed Q-learning}
Delayed Q-learning differs from traditional Q-learning in a few ways.  Rather than updating the Q-values at every time step, the algorithm updates $Q(s,a)$ when a state-action pair $(s,a)$ has been executed $m$ times since its last update.  It also uses a different update function:
$$Q_{t+1}(s,a) = \frac{1}{m}\sum_{i=1}^m(r_{k_i}+\gamma V_{k_i}(s_{k_i})) + \epsilon_1$$  
where $r_i$ is the $i$th reward received and $s_{k_1},\dots,s_{k_m}$ are the $m$ most recent next-states observed form executing $(s,a)$ at times $k_1 < \dots < k_m = t$, respectively. The user defines $\epsilon_1$ and $m$.   
% We will have some of these vars defined from earlier so I want define them all here
After $m$ executions of $(s,a)$ an update will only occur if the new Q-value estimate is at least $\epsilon_1$ smaller than the previous estimate.  If it is not smaller, then updates of $Q(s,a)$ are no longer allowed until another Q-value estimate (for a different state-action pair) is updated.

Waiting for $m$ samples before updating has an averaging effect that helps minimize any randomness. Adding $\epsilon_1$ to the update function helps achieve optimism which in turn facilitates safe exploration. By starting with high initial Q-values and only allowing Q-values to decrease ensures that each state is regularly visited and that each action is tried from every state.  This makes sure each state-action pair receives enough samples to converge to the correct Q-value.%<- discuss optimism more?
In the following section, we outline the proof that shows Delayed Q-learning is PAC-MDP.


\subsection{Proof Sketch: Delayed Q-learning is PAC-MDP}

Should probably explain how PAC-MDP is different than PAC?


The number of successful Q-value updates for a particular $(s,a)$ is bounded by

$$\kappa = \frac{1}{(1-\gamma)\epsilon_1}.$$

This follows from the fact that $Q(s,a)$ is initialized to $1/(1-\gamma)$ and that every $Q(s,a)$ results in a decrease of at least $\epsilon_1$ (and it is assumed rewards are non-negative).  For $A$ total actions and $S$ total states, at most $SA\kappa$ updates can occur.  Now, a state-action pair $(s,a)$ is initially allowed to attempt an update.  Then, a successful update of at most one other Q-value must occur  for $(s,a)$ to attempt another update.  So, there can be at most $1+SA\kappa$ attempted updates of $(s,a)$.  Therefore, there are at most
$$SA(1+SA\kappa)$$
total attempted updates.

During timestep $t$ of learning define $K_t$ to be the set of all state-action pairs $(s,a)$ such that 

$$Q_t(s,a) - \left(R(s,a)+\gamma\sum_{s'}T(s'|s,a)V_t(s')\right) \leq 3\epsilon_1.$$

A value for $m$ is specified as 

$$m=\frac{\ln{3SA(1+SA\kappa)/\delta}}{2\epsilon_1^2(1-\gamma)^2}.$$

Strehl et al. prove the following lemmas:

\begin{enumerate}

\item Consider the following assumption:  Suppose an attempted update of state-action pair $(s,a)$ occurs at time $t$, and that the $m$ most recent experiences of $(s,a)$ happened at times $k_1 < \dots < k_m = t$.  If $(s,a) \notin K_{k_1}$, then the attempted update will be successful.  The probability that this assumption is violated is at most $\delta/3$.  

% A proof sketch is given for this in Strehl

\item During the execution of Delayed Q-learning, $Q_t(s,a) \geq Q^*(s,a)$ holds for all timesteps $t$ and state-action pairs $(s,a)$ with probability at least $1-\delta/3$.

\item The number of timesteps $t$ such that a state-action pair $(s,a) \notin K_t$ is experienced is at most $2mSA\kappa$.

\item Let $M$ be an MDP, $K$ a set of state-action pairs, $M'$ an MDP equal to $M$ on $K$ (identical transition and reward functions), $\pi$ a policy, and
$T$ some positive integer. Let $A_M$ be the event that a state-action pair not in $K$ is encountered in a trial generated by starting from state $s$ and following $\pi$ for $T$ timesteps in $M$. Then,
$$V_M^\pi(s,T)\geq V_{M'}^{\pi}(s,T)-\text{Pr}(A_M)/(1-\gamma).$$

\end{enumerate}

Suppose Delayed Q-learning is run on an MDP $M$. It is assumed that the assumption in lemma 1 holds and that $Q_t(s,a) \geq Q^*(s,a)$.  These assumptions are broken with probability at most $2\delta/3$ by lemmas 1 and 2.  
Let $T=O\left(\frac{1}{1-\gamma}\ln{\frac{1}{\epsilon_2(1-\gamma)}}\right)$ be large enough so that $|V_{M'}^{\pi_t}(s_t,T) - V_{M'}^{\pi_t}(s_t)| \leq \epsilon_2$.   Let $\text{Pr}(U)$ denote the probability of
the algorithm performing a successful update on some
state-action pair $(s, a)$, while executing policy
$\mathcal{A}_t$ from state $s_t$ in $M$ for $T$ timesteps. From the previous statements and lemma 5 we have 
\begin{align*}
V_M^{\mathcal{A}_t} & \geq V_{M'}^{\mathcal{A}_t}(s_t,T)-\text{Pr}(A_M)/(1-\gamma) \\
 & \geq V_{M'}^{\pi_t}(s_t,T)-\text{Pr}(A_M)/(1-\gamma) -\text{Pr}(U)/(1-\gamma) \\
 & \geq V_{M'}^{\pi_t}(s_t)-\epsilon_2-(\text{Pr}(A_M)/(1-\gamma) +\text{Pr}(U)/(1-\gamma)). 
\end{align*}



More steps to be explained in between....

The number of timesteps needed for 
$$V_{\mathcal{A}_t}^{\pi_t}(s_t,T) \geq V^*(s_t)-\epsilon$$
is 
$$O\left(\frac{SA}{\epsilon^4(1-\gamma)^8}\ln{\frac{1}{\delta}}\ln{\frac{1}{\epsilon(1-\gamma)}}\ln{\frac{SA}{\delta\epsilon(1-\gamma)}}\right)$$
So, Delayed Q-learning will follow an $\epsilon$-optimal policy on all but $O\left(\frac{SA}{\epsilon^4(1-\gamma)^8}\ln{\frac{1}{\delta}}\ln{\frac{1}{\epsilon(1-\gamma)}}\ln{\frac{SA}{\delta\epsilon(1-\gamma)}}\right)$ steps with probability at least $1-\delta$.  Thus, it is PAC-MDP.
\\ 
\\
$R_{max}$ is another reinforcement learning algorithm proved to be PAC-MDP that we can discuss.

\nocite{yang_multiagent_2004}
\nocite{kim_autonomous_2003}
\nocite{mohri_foundations_2012}

\newpage
\bibliography{ML}{}
\bibliographystyle{plain}




% --------------------------------------------------------------
% You don't have to mess with anything below this line.
% --------------------------------------------------------------
\end{document}
