% --------------------------------------------------------------
% This is all preamble stuff that you don't have to worry about.
% Head down to where it says "Start here"
% --------------------------------------------------------------
\documentclass[10pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amsthm,amssymb}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}
\usepackage{verbatim}
\usepackage{color}
\usepackage[toc,page]{appendix}



\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newenvironment{theorem}[2][Theorem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{lemma}[2][Lemma]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{exercise}[2][Exercise]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{problem}[2][Problem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{question}[2][Question]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{corollary}[2][Corollary]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\begin{document}
% --------------------------------------------------------------
% Start here
% --------------------------------------------------------------
\title{Progress Report: Applications of Reinforcement Learning in Robotics and Optimal Control}%replace X with the appropriate number
\author{\\ %replace with your name
Kristopher L. Reynolds\\
Matthew C. Sheckells
\\} %if necessary, replace with your course title
\maketitle
\section{Introduction}
Concepts of machine learning are very much at the forefront of modern research interests. However, the popular supervised learning framework does not account for environments that are constantly in flux. This setting is relevant to autonomous underwater vehicles navigating unknown terrains, or fault detection in teams of swarming land-based robots. A notion that suggests a means to handle these changing environments is reinforcement learning (RL). Unlike supervised learning, RL actively receives information by inducing an \textit{environment} through \textit{actions}. By doing so, two types of information are rendered: the current \textit{state}, and some notion of a quantifiable \textit{reward}. This reward is related to an a priori defined task that has a corresponding goal. RL aims to learn reward values for taking a particular action from a particular state, so that a system can learn to act optimally in a given situation.

The fields of robotics and optimal control have benefitted from concepts of reinforcement learning \cite{kober_reinforcement_2013}\cite{kaelbling_reinforcement_1996}. Several robotic systems in  \cite{bhasin_reinforcement_2011} and \cite{hester_rtmba:_2012} have implemented this technique, showing that performance equalled and in some cases exceeded that of hand-tuned controllers. The advantage of RL is that it is robust to changes in a system, such as malfunctioning actuators, changes in models parameters, and external disturbances. Optimal control is similar in several ways to RL. Moreover, both have some notion of finding a \textit{policy} that optimizes some objective function. For optimal control, this is minimizing some cost, whereas for RL, it is maximizing a reward. For robotics-based RL, there are several challenges that need to be addressed for RL to be realized.
\begin{itemize}
\item Continuous State and Action Space: Robots have sensors that output data in a continuous domain.  Likewise, robots themselves act in a continuous environment. This issue is addressed in \cite{gaskett_thesis} and \cite{q_learning_navigation}.
\item Under-modeling and Uncertainty: The full state may not be observable, and noise requires some method of state estimation.
\item Goal specification: Crafting reward functions (i.e. \textit{reward shaping}) may be difficult since rewards can rely upon successful completion of a task. This may lead to a sparsity of instances actually influencing the learning of a robotic system.
\end{itemize}

In this work, we will discuss a common reinforcement learning technique, called Q-learning.  We will show that a variant of it called Delayed Q-learning is PAC learnable.  Then, we will give an overview of how the challenges mentioned above are addressed when implementing RL algorithms on real systems.  Additionally, empirical demonstrations of a successful robotics-based RL implementation will be shown. 
% ^Figured the first paragraph of the proposal could be used as a starting point
% We can discuss all of our references in more detail here



\section{What has been done}
\begin{itemize}
\item Introduction: Presentation of the RL framework, with definitions of reward, policy, Markov Decision Processes (MDPs). Advantages of RL are also discussed.
\item Discussed traditional Q-learning and Delayed Q-learning.
\item Shown Delayed Q-learning is PAC-MDP-learnable.
\end{itemize}


\section{What remains to be done}
\begin{itemize}
\item Survey of reinforcement learning in robotics and optimal control. 
\item Validate RL in simulation and/or real hardware.
\end{itemize}

\nocite{yang_multiagent_2004}
\nocite{kim_autonomous_2003}
\nocite{mohri_foundations_2012}


\newpage
\bibliography{ML}{}
\bibliographystyle{plain}




% --------------------------------------------------------------
% You don't have to mess with anything below this line.
% --------------------------------------------------------------
\end{document}