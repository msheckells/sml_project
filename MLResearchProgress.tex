% --------------------------------------------------------------
% This is all preamble stuff that you don't have to worry about.
% Head down to where it says "Start here"
% --------------------------------------------------------------
\documentclass[10pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amsthm,amssymb}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}
\usepackage{verbatim}
\usepackage{color}
\usepackage[toc,page]{appendix}



\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newenvironment{theorem}[2][Theorem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{lemma}[2][Lemma]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{exercise}[2][Exercise]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{problem}[2][Problem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{question}[2][Question]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{corollary}[2][Corollary]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\begin{document}
% --------------------------------------------------------------
% Start here
% --------------------------------------------------------------
\title{Progress Report: Applications of Reinforcement Learning in Robotics and Optimal Control}%replace X with the appropriate number
\author{\\ %replace with your name
Kristopher L. Reynolds\\
Matthew C. Sheckells
\\} %if necessary, replace with your course title
\maketitle
\section{Introduction}
Concepts of machine learning are very much at the forefront of modern research interests. However, the popular supervised learning framework does not account for environments that are constantly in flux. This setting is relevant to Autonomous underwater vehicles navigating unknown terrains, or or fault detection in teams of swarming land-based robots. A notion that suggests a means to handle these changing environments is reinforcement learning (RL). Unlike supervised learning, RL actively receives information by inducing an \textit{environment} through \textit{actions}. By doing so, two types of information are rendered: the current \textit{state}, and some notion of a quantifiable \textit{reward}. This reward is related to an a priori defined task that has a corresponding goal. 
\\
\\
The fields of robotics and optimal control have benefitted from concepts of reinforcement learning\cite{kober_reinforcement_2013} \cite{kaelbling_reinforcement_1996}, with several robotic systems in  \cite{bhasin_reinforcement_2011} and \cite{hester_rtmba:_2012} having already implemented this technique, showing that performance equalled and in some cases exceeded that of hand-tuned controllers. The advantage of RL is that it is robust to changes in a system, such as malfunctioning actuators, changes in models parameters (i.e. inertia), and external disturbances. This reports aims to balance theoretical results with empirical demonstrations of successful RL implementation. 
% ^Figured the first paragraph of the proposal could be used as a starting point
% We can discuss all of our references in more detail here



\section{What has been done}
\begin{itemize}
\item Introduction: Presentation of the RL framework, with definitions of reward, policy, Markov Decision Processes (MDPs). Advantages of RL are also discussed.
\item Discussed traditional Q-learning and Delayed Q-learning
\item Shown Delayed Q-learning is PAC-MDP-learnable
\end{itemize}


\section{What remains to be done}
\begin{itemize}
\item Survey of reinforcement learning in robotics and optimal control 
\item Validate RL in simulation and/or real hardware
\end{itemize}

\nocite{yang_multiagent_2004}
\nocite{kim_autonomous_2003}
\nocite{mohri_foundations_2012}


\newpage
\bibliography{ML}{}
\bibliographystyle{plain}




% --------------------------------------------------------------
% You don't have to mess with anything below this line.
% --------------------------------------------------------------
\end{document}