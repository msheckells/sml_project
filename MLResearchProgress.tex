% --------------------------------------------------------------
% This is all preamble stuff that you don't have to worry about.
% Head down to where it says "Start here"
% --------------------------------------------------------------
\documentclass[10pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amsthm,amssymb}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}
\usepackage{verbatim}
\usepackage{color}
\usepackage[toc,page]{appendix}



\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newenvironment{theorem}[2][Theorem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{lemma}[2][Lemma]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{exercise}[2][Exercise]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{problem}[2][Problem]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{question}[2][Question]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\newenvironment{corollary}[2][Corollary]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries #2.}]}{\end{trivlist}}
\begin{document}
% --------------------------------------------------------------
% Start here
% --------------------------------------------------------------
\title{Progress Report: Applications of Reinforcement Learning in Robotics and Optimal Control}%replace X with the appropriate number
\author{\\ %replace with your name
Kristopher L. Reynolds\\
Matthew C. Sheckells
\\} %if necessary, replace with your course title
\maketitle
\section{Introduction}
Concepts of machine learning are very much at the forefront of modern research interests. However, the popular supervised learning framework does not account for environments that are constantly in flux. This setting is relevant to Autonomous underwater vehicles navigating unknown terrains, or or fault detection in teams of swarming land-based robots. A notion that suggests a means to handle these changing environments is reinforcement learning (RL). Unlike supervised learning, RL actively receives information by inducing an \textit{environment} through \textit{actions}. By doing so, two types of information are rendered: the current \textit{state}, and some notion of a quantifiable \textit{reward}. This reward is related to an a priori defined task that has a corresponding goal. 
\\
\\
The fields of robotics and optimal control have benefitted from concepts of reinforcement learning\cite{kober_reinforcement_2013} \cite{kaelbling_reinforcement_1996}, with several robotic systems in  \cite{bhasin_reinforcement_2011} and \cite{hester_rtmba:_2012} having already implemented this technique, showing that performance equalled and in some cases exceeded that of hand-tuned controllers. The advantage of RL is that it is robust to changes in a system, such as malfunctioning actuators, changes in models parameters (i.e. inertia), and external disturbances. Optimal control is similar in several ways to RL. Moreover, both have some notion of finding a \textit{policy} that optimizes some objective function. For optimal control, this is minimizing some cost, whereas for RL, it is maximizing a reward. For robotics-based RL, there are several challenges that need to be addressed for RL to be realized.
\begin{itemize}
\item Representing continuous high dimensional states and actions through acceptable resolution.
\item Real-world sampling: consider that robots have wear and tear and for safety reasons require human supervision.
\item Under-modeling and uncertainty: The full state may not be observable, and noise requires some method of state estimation.
\item Goal specification: crafting reward functions (i.e. \textit{reward shaping}) may be difficult since rewards can rely upon successful completion of a task. This may lead to a sparsity of instances actually influencing the learning of a robotic system.
\end{itemize}
Having now addressed some of the challenges in robotics-based RL, this project will aim to balance theoretical results with empirical demonstrations of successful robotics-based RL implementation/(s). 
% ^Figured the first paragraph of the proposal could be used as a starting point
% We can discuss all of our references in more detail here



\section{What has been done}
\begin{itemize}
\item Introduction: Presentation of the RL framework, with definitions of reward, policy, Markov Decision Processes (MDPs). Advantages of RL are also discussed.
\item Discussed traditional Q-learning and Delayed Q-learning.
\item Shown Delayed Q-learning is PAC-MDP-learnable.
\end{itemize}


\section{What remains to be done}
\begin{itemize}
\item Survey of reinforcement learning in robotics and optimal control. 
\item Validate RL in simulation and/or real hardware.
\end{itemize}

\nocite{yang_multiagent_2004}
\nocite{kim_autonomous_2003}
\nocite{mohri_foundations_2012}


\newpage
\bibliography{ML}{}
\bibliographystyle{plain}




% --------------------------------------------------------------
% You don't have to mess with anything below this line.
% --------------------------------------------------------------
\end{document}