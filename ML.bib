
@article{q_learning_navigation,
	title = {Mobile robot Navigation Based
on Q-Learning Technique },
	urldate = {2015-02-17},
	journal = {International Journal of Advanced Robotic Systems},
	author = {Lazhar Khriji, Farid Touati, Kamel Benhmed and Amur Al-Yahmedi},
	volume = {8},
	year = {2011},
	pages = {45-51}
}

@article{watkins,
year={1992},
issn={0885-6125},
journal={Machine Learning},
volume={8},
number={3-4},
doi={10.1007/BF00992698},
title={Q-learning},
url={http://dx.doi.org/10.1007/BF00992698},
publisher={Kluwer Academic Publishers},
keywords={Q-learning; reinforcement learning; temporal differences; asynchronous dynamic programming},
author={Watkins, Christopher J.C.H. and Dayan, Peter},
pages={279-292},
language={English}
}


@book{mohri,
 author = {Mohri, Mehryar and Rostamizadeh, Afshin and Talwalkar, Ameet},
 title = {Foundations of Machine Learning},
 year = {2012},
 isbn = {026201825X, 9780262018258},
 publisher = {The MIT Press},
} 

@article{deep_rl,
  author    = {Volodymyr Mnih and
               Koray Kavukcuoglu and
               David Silver and
               Alex Graves and
               Ioannis Antonoglou and
               Daan Wierstra and
               Martin A. Riedmiller},
  title     = {Playing Atari with Deep Reinforcement Learning},
  journal   = {CoRR},
  volume    = {abs/1312.5602},
  year      = {2013},
  url       = {http://arxiv.org/abs/1312.5602},
  timestamp = {Wed, 01 Apr 2015 20:06:15 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/MnihKSGAWR13},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@article{carden2014,
  author    = {Stephen Carden},
  title     = {Convergence of a Q-learning Variant for Continuous States and Actions},
  journal   = {Journal of Artificial Intelligence Research},
  volume    = {49},
  year      = {2014},
  url       = {https://www.jair.org/media/4271/live-4271-7865-jair.pdf}
}

@article{nadaraya,
author = {E. A. Nadaraya},
title = {On Estimating Regression},
journal = {Theory of Probability \& Its Applications},
volume = {9},
number = {1},
pages = {141-142},
year = {1964},
doi = {10.1137/1109020},
URL = {http://dx.doi.org/10.1137/1109020}

}

@article{lin,
year={1992},
issn={0885-6125},
journal={Machine Learning},
volume={8},
number={3-4},
doi={10.1007/BF00992699},
title={Self-improving reactive agents based on reinforcement learning, planning and teaching},
url={http://dx.doi.org/10.1007/BF00992699},
publisher={Kluwer Academic Publishers},
keywords={Reinforcement learning; planning; teaching; connectionist networks},
author={Lin, Long-Ji},
pages={293-321},
language={English}
}

@phdthesis{kakade,
 	title = {On the Sample Complexity of Reinforcement Learning},
	author = {Sham Kakade},
	school = {University College London},
	year = {2003} 
}

@phdthesis{gaskett_thesis,
	title = {Q-Learning for Robot Control},
	school = {Australian National University},
	author = {Chris Gaskett},
	year = {2002}
}

@phdthesis{ng_thesis,
	title = {Shaping and policy search in Reinforcement Learning},
	school = {University of California, Berkeley},
	author = {Andrew Y. Ng},
	year = {2003}
}

@article{kober_reinforcement_2013,
	title = {Reinforcement learning in robotics: {A} survey},
	shorttitle = {Reinforcement learning in robotics},
	url = {http://ijr.sagepub.com/content/early/2013/08/22/0278364913495721.abstract},
	urldate = {2015-02-17},
	journal = {The International Journal of Robotics Research},
	author = {Kober, Jens and Bagnell, J. Andrew and Peters, Jan},
	year = {2013}
	file = {Snapshot:/Users/kristopherreynolds1/Library/Application Support/Firefox/Profiles/a07yfnax.default/zotero/storage/R42JRB6M/0278364913495721.html:text/html}
}

@inproceedings{andersson_model-based_2015,
	title = {Model-{Based} {Reinforcement} {Learning} in {Continuous} {Environments} {Using} {Real}-{Time} {Constrained} {Optimization}},
	url = {http://www.diva-portal.org/smash/record.jsf?pid=diva2:781572},
	urldate = {2015-02-19},
	booktitle = {Twenty-{Ninth} {AAAI} {Conference} on {Artificial} {Intelligence} ({AAAI}15)},
	author = {Andersson, Olov and Heintz, Fredrik and Doherty, Patrick},
	year = {2015},
	file = {Snapshot:/Users/kristopherreynolds1/Library/Application Support/Firefox/Profiles/a07yfnax.default/zotero/storage/J9MRG5F3/record.html:text/html}
}

@inproceedings{sutton,
	title = {Reinforcement Learning is Direct Adaptive Optimal Control},
	author = {Sutton, R., Barto A., and Williams, R.},
	booktitle = {American Control Conference},
	year = {1991}

}

@inproceedings{hester_rtmba:_2012,
	title = {{RTMBA}: {A} {Real}-{Time} {Model}-{Based} {Reinforcement} {Learning} {Architecture} for robot control},
	shorttitle = {{RTMBA}},
	doi = {10.1109/ICRA.2012.6225072},
	abstract = {Reinforcement Learning (RL) is a paradigm for learning decision-making tasks that could enable robots to learn and adapt to their situation on-line. For an RL algorithm to be practical for robotic control tasks, it must learn in very few samples, while continually taking actions in real-time. Existing model-based RL methods learn in relatively few samples, but typically take too much time between each action for practical on-line learning. In this paper, we present a novel parallel architecture for model-based RL that runs in real-time by 1) taking advantage of sample-based approximate planning methods and 2) parallelizing the acting, model learning, and planning processes in a novel way such that the acting process is sufficiently fast for typical robot control cycles. We demonstrate that algorithms using this architecture perform nearly as well as methods using the typical sequential architecture when both are given unlimited time, and greatly out-perform these methods on tasks that require real-time actions such as controlling an autonomous vehicle.},
	booktitle = {2012 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
	author = {Hester, T. and Quinlan, M. and Stone, P.},
	month = may,
	year = {2012},
	keywords = {Approximation algorithms, approximation theory, autonomous vehicle, Computational modeling, control engineering computing, decision making, decision-making learning, learning (artificial intelligence), Multicore processing, parallel architecture, parallel architectures, Planning, planning processes, real-time model-based reinforcement learning architecture, real-time systems, Real time systems, RL, robot control, robots, RTMBA, sample based approximate planning methods},
	pages = {85--90},
	file = {IEEE Xplore Abstract Record:/Users/kristopherreynolds1/Library/Application Support/Firefox/Profiles/a07yfnax.default/zotero/storage/BEMS4JKI/abs_all.html:text/html;IEEE Xplore Full Text PDF:/Users/kristopherreynolds1/Library/Application Support/Firefox/Profiles/a07yfnax.default/zotero/storage/75K6JFB6/Hester et al. - 2012 - RTMBA A Real-Time Model-Based Reinforcement Learn.pdf:application/pdf}
}

@phdthesis{bhasin_reinforcement_2011,
	title = {Reinforcement learning and optimal control methods for uncertain nonlinear systems},
	url = {http://ncr.mae.ufl.edu/dissertations/bhasin.pdf},
	urldate = {2015-02-19},
	school = {University of Florida},
	author = {Bhasin, Shubhendu},
	year = {2011},
	file = {[PDF] from ufl.edu:/Users/kristopherreynolds1/Library/Application Support/Firefox/Profiles/a07yfnax.default/zotero/storage/ITHSFV97/Bhasin - 2011 - Reinforcement learning and optimal control methods.pdf:application/pdf}
}

@techreport{yang_multiagent_2004,
	title = {Multiagent reinforcement learning for multi-robot systems: {A} survey},
	shorttitle = {Multiagent reinforcement learning for multi-robot systems},
	url = {http://computerscience.nl/docs/vakken/aibop/MAS-reinforcement-learning.pdf},
	urldate = {2015-02-19},
	institution = {tech. rep},
	author = {Yang, Erfu and Gu, Dongbing},
	year = {2004},
	file = {[PDF] from computerscience.nl:/Users/kristopherreynolds1/Library/Application Support/Firefox/Profiles/a07yfnax.default/zotero/storage/ZKMTHSAS/Yang and Gu - 2004 - Multiagent reinforcement learning for multi-robot .pdf:application/pdf}
}

@article{kaelbling_reinforcement_1996,
	title = {Reinforcement learning: {A} survey},
	shorttitle = {Reinforcement learning},
	url = {http://www.jair.org/papers/paper301.html},
	urldate = {2015-02-19},
	journal = {Journal of artificial intelligence research},
	author = {Kaelbling, Leslie Pack and Littman, Michael L. and Moore, Andrew W.},
	year = {1996},
	pages = {237--285},
	file = {[PDF] from jair.org:/Users/kristopherreynolds1/Library/Application Support/Firefox/Profiles/a07yfnax.default/zotero/storage/HV797TZX/Kaelbling et al. - 1996 - Reinforcement learning A survey.pdf:application/pdf;Snapshot:/Users/kristopherreynolds1/Library/Application Support/Firefox/Profiles/a07yfnax.default/zotero/storage/HK5Z7IXE/paper301.html:text/html}
}

@inproceedings{strehl_pac_2006,
	title = {{PAC} model-free reinforcement learning},
	url = {http://dl.acm.org/citation.cfm?id=1143955},
	urldate = {2015-02-19},
	booktitle = {Proceedings of the 23rd international conference on {Machine} learning},
	publisher = {ACM},
	author = {Strehl, Alexander L. and Li, Lihong and Wiewiora, Eric and Langford, John and Littman, Michael L.},
	year = {2006},
	pages = {881--888},
	file = {[PDF] from wustl.edu:/Users/kristopherreynolds1/Library/Application Support/Firefox/Profiles/a07yfnax.default/zotero/storage/XDTP43XQ/Strehl et al. - 2006 - PAC model-free reinforcement learning.pdf:application/pdf;Snapshot:/Users/kristopherreynolds1/Library/Application Support/Firefox/Profiles/a07yfnax.default/zotero/storage/AN3V987P/citation.html:text/html}
}

@inproceedings{quigley_ros:_2009,
	title = {{ROS}: an open-source {Robot} {Operating} {System}},
	volume = {3},
	shorttitle = {{ROS}},
	url = {https://www.willowgarage.com/sites/default/files/icraoss09-ROS.pdf},
	urldate = {2015-02-19},
	booktitle = {{ICRA} workshop on open source software},
	author = {Quigley, Morgan and Conley, Ken and Gerkey, Brian and Faust, Josh and Foote, Tully and Leibs, Jeremy and Wheeler, Rob and Ng, Andrew Y.},
	year = {2009},
	pages = {5},
	file = {[PDF] from willowgarage.com:/Users/kristopherreynolds1/Library/Application Support/Firefox/Profiles/a07yfnax.default/zotero/storage/HFB4AX36/Quigley et al. - 2009 - ROS an open-source Robot Operating System.pdf:application/pdf}
}

@inproceedings{kim_autonomous_2003,
	title = {Autonomous helicopter flight via reinforcement learning},
	url = {http://machinelearning.wustl.edu/mlpapers/paper_files/NIPS2003_CN07.pdf},
	urldate = {2015-02-19},
	booktitle = {Advances in neural information processing systems},
	author = {Kim, H. J. and Jordan, Michael I. and Sastry, Shankar and Ng, Andrew Y.},
	year = {2003},
	file = {[PDF] from wustl.edu:/Users/kristopherreynolds1/Library/Application Support/Firefox/Profiles/a07yfnax.default/zotero/storage/GPKZAM93/Kim et al. - 2003 - Autonomous helicopter flight via reinforcement lea.pdf:application/pdf}
}

@inproceedings{takahashi,
	title = {Continuous valued Q-learning for
vision-guided behavior},
	booktitle = {IEEE/SICE/RSJ International Conference on Multisensor Fusion and Integration for Intelligent Systems},
	author = {Takahashi, Y., Takeda, M., and Asada, M.},
	year = {1999}
}

@incollection{ng_autonomous_2006,
	title = {Autonomous inverted helicopter flight via reinforcement learning},
	url = {http://link.springer.com/chapter/10.1007/11552246_35},
	urldate = {2015-02-19},
	booktitle = {Experimental {Robotics} {IX}},
	publisher = {Springer},
	author = {Ng, Andrew Y. and Coates, Adam and Diel, Mark and Ganapathi, Varun and Schulte, Jamie and Tse, Ben and Berger, Eric and Liang, Eric},
	year = {2006},
	pages = {363--372},
	file = {[PDF] from stanford.edu:/Users/kristopherreynolds1/Library/Application Support/Firefox/Profiles/a07yfnax.default/zotero/storage/X2MRRRJA/Ng et al. - 2006 - Autonomous inverted helicopter flight via reinforc.pdf:application/pdf;Snapshot:/Users/kristopherreynolds1/Library/Application Support/Firefox/Profiles/a07yfnax.default/zotero/storage/MISF4GKR/11552246_35.html:text/html}
}

@book{lqr,
	author = {B. D. O. Anderson and J. B. Moore},
	title = {Optimal Control: Linear Quadratic Methods},
	publisher = {Prentice-Hall},
	year = {1989}

}